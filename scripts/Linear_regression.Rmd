---
title: "linear_regression"
author: "katherine"
date: "2/8/2020"
output: html_document
---

```{r library}
library(tidyverse)
library(glmnet)
library(RColorBrewer)
library(Metrics)
mypalette <- brewer.pal(6,"Set1")
```

Read Dataset
```{r read dataset}
potatorar = read.csv("potatometadata_forClaudia.csv")

exploratory = potatorar[which(potatorar$Experiment == 2015), ]
confirmatory  = potatorar[which(potatorar$Experiment == 2017), ]
```

Rearranging the tables
```{r}
nu = data.frame("nutrients" = c("high", "low"))
nu = data.frame("nutrients" = nu[rep(seq_len(nrow(nu)), each = 13), ])
t1 =  potatorar[, -which(colnames(potatorar)=="Mean.LowN.Microbe")]
t2 =  potatorar[, -which(colnames(potatorar)=="Mean.Control.Microbe")]

t3 =  t1[, -which(colnames(t1)=="varNtub")]
t4 =  t2[, -which(colnames(t2)=="varCtub")]

t5 = t3[, -which(colnames(t3)=="Std.Err.Mean.LowN.Microbe")]  
t6 =  t4[, -which(colnames(t4)=="Std.Err.Mean.Control.Microbe")]

t7 = t5[, -which(colnames(t5)=="invVar_Ntub")]  
t8 =  t6[, -which(colnames(t6)=="invVar_Ctub")]

c1 =  cbind(t7,data.frame("nutrients" = nu[1:13,]))
names(c1)[names(c1) == "Mean.Control.Microbe"] <- "yields_average"
names(c1)[names(c1) == "varCtub"] <- "yields_variance"
names(c1)[names(c1) == "Std.Err.Mean.Control.Microbe"] <- "yields_std.err.mean"
names(c1)[names(c1) == "invVar_Ctub"] <- "yields_inverse.var"

c2 = cbind(t8,data.frame("nutrients" = nu[14:26,]))
names(c2)[names(c2) == "Mean.LowN.Microbe"] <- "yields_average"
names(c2)[names(c2) == "varNtub"] <- "yields_variance"
names(c2)[names(c2) == "Std.Err.Mean.LowN.Microbe"] <- "yields_std.err.mean"
names(c2)[names(c2) == "invVar_Ntub"] <- "yields_inverse.var"

potatorar_update = rbind(c1, c2)
potatorar_update = potatorar_update %>% arrange(potatorar_update$FieldID)
write.csv(potatorar_update, file = "potatorar_update.csv", quote = FALSE)
```

Get data for two different experiments
```{r}
exploratory = potatorar_update[which(potatorar_update$Experiment == 2015), ]
confirmatory  = potatorar_update[which(potatorar_update$Experiment == 2017), ]
```

```{r}
claderich_e = exploratory[, grep("claderich",colnames(exploratory))]
claderich_e=cbind(exploratory$FieldID,claderich_e)

claderich_c = confirmatory[, grep("claderich",colnames(confirmatory))]
claderich_c=cbind(confirmatory$FieldID,claderich_c)

cladediv_e = exploratory[, grep("cladediv",colnames(exploratory))]
cladediv_e=cbind(exploratory$FieldID,cladediv_e)

cladediv_c = confirmatory[, grep("cladediv",colnames(confirmatory))]
cladediv_c=cbind(confirmatory$FieldID,cladediv_c)
```

Lasso 2015

Split the data
```{r}
set.seed(86)
Xmat <- as.matrix(cbind(cladediv_e[,grep("cladediv",colnames(cladediv_e))],                            claderich_e[,grep("claderich",colnames(claderich_e))]))
Ymat <- potatorar_update$yields_average [which(potatorar_update$Experiment == 2015)]
train = sample(1:nrow(Xmat), nrow(Xmat)/2)
test = (-train)
x_test = as.matrix(Xmat[test, ])
x_train = as.matrix(Xmat[train,])
y_test = Ymat[test]
y_train = Ymat[train]
```

identifying best lamda
```{r}
set.seed(86)
potatorar.lasso.cv <- cv.glmnet(Xmat, Ymat, nfold=5)
opt_lambda = potatorar.lasso.cv$lambda.min
```

Using this value, let us train the lasso model again.
Checking the obs
```{r}
set.seed(86)
potatorar.lasso<- glmnet(Xmat, Ymat, alpha = 1, lambda = opt_lambda)
```

Inspecting beta coefficients
```{r}
coef(potatorar.lasso)
```

Lasso 2017

Split the data
```{r}
set.seed(86)
Xmat_17 <- as.matrix(cbind(cladediv_c[,grep("cladediv",colnames(cladediv_c))],                            claderich_c[,grep("claderich",colnames(claderich_c))]))
Ymat_17 <- potatorar_update$yields_average [which(potatorar_update$Experiment == 2017)]
                  
train_17 = sample(1:nrow(Xmat_17), nrow(Xmat_17)/2)
test_17 = (-train_17)
x_test_17 = as.matrix(Xmat_17[test_17, ])
x_train_17 = as.matrix(Xmat_17[train_17,])
y_test_17 = Ymat_17[test_17]
y_train_17 = Ymat_17[train_17]
```

identifying best lamda
```{r}
set.seed(86)
potatorar.lasso.cv_17 <- cv.glmnet(x_train_17, y_train_17, nfold=5)
plot(potatorar.lasso.cv_17)
opt_lambda_17 = potatorar.lasso.cv_17$lambda.min
```

Using this value, let us train the lasso model again.
Checking the obs
```{r}
potatorar.lasso_17<- glmnet(x_train_17, y_train_17, lambda = opt_lambda_17)
pred_17 <- predict(potatorar.lasso_17, s = opt_lambda_17, newx = x_test_17)
final_17 <- cbind(y_test_17, pred_17)
final_17
```

Inspecting beta coefficients
```{r}
coef(potatorar.lasso_17)
```

ALL lasso

```{r}
set.seed(86)
Xmat_all <- as.matrix(potatorar_update[,grep("clade",colnames(potatorar_update))])
Ymat_all <- potatorar_update$yields_average 
train_all = sample(1:nrow(Xmat_all), nrow(Xmat_all)/2)
test_all = (-train_all)
x_test_all = as.matrix(Xmat_all[test_all, ])
x_train_all = as.matrix(Xmat_all[train_all,])
y_test_all = Ymat_all[test_all]
y_train_all = Ymat_all[train_all]
y_train_all
```

```{r}
set.seed(86)
potatorar.lasso.cv_all <- cv.glmnet(x_train_all, y_train_all, nfold=5)
plot(potatorar.lasso.cv_all)
opt_lambda_all = potatorar.lasso.cv_all$lambda.min
```

```{r}
potatorar.lasso_all<- glmnet(x_train_all, y_train_all, lambda = opt_lambda_all)
pred_all <- predict(potatorar.lasso_all, s = opt_lambda_all, newx = x_test_all)
final_all <- cbind(y_test_all, pred_all)
final_all
```

```{r}
coef(potatorar.lasso_all)
```

```{r}
#MSE
sum_MSE = sum((y_test_all - pred_all)^2)
#AVG - test mean square error
mse_test_value <- mean((y_test_all - pred_all)^2)
mse_test_value
```


linear regression
```{r}
Xmat_regre<- as.matrix(potatorar_update[,c(5,grep("clade",colnames(potatorar_update)))])
train_regre = sample(1:nrow(Xmat_regre), nrow(Xmat_regre)/2)
test_regre = (-train_regre)
test_regre =as.matrix(Xmat_regre[test_regre, ])
test_regre = as.data.frame(test_regre)
train_regre = as.matrix(Xmat_regre[train_regre,])
train_regre = as.data.frame(train_regre)
```

```{r}
lm_potato <- lm(yields_average ~ cladediv0.1+cladediv0.8+claderich0.45+cladediv0.25+claderich0.2+cladediv0.3+cladediv0.45, data=train_regre)
summary(lm_potato)
```


```{r}
pred_regre = predict(lm_potato, test_regre)
#MSE
sum_regre = sum((test_regre$yields_average - pred_regre)^2)
#AVG - test mean square error
mse_regre_value <- mean((test_regre$yields_average - pred_regre)^2)
mse_regre_value
```

Lasso using selected features
```{r}
selected_features = c("cladediv0.1","cladediv0.8","claderich0.45","cladediv0.25","claderich0.2","cladediv0.3","cladediv0.45")
Xmat_selected = as.matrix(potatorar_update[,selected_features])
Ymat_selected <- potatorar_update$yields_average
                  
train_selected = sample(1:nrow(Xmat_selected ), nrow(Xmat_selected )/2)
test_selected  = (-train_selected )
x_test_selected  = as.matrix(Xmat_selected [test_selected , ])
x_train_selected  = as.matrix(Xmat_selected [train_selected ,])
y_test_selected  = Ymat_selected[test_selected ]
y_train_selected  = Ymat_selected[train_selected ]
train_selected
```

```{r}
set.seed(86)
potatorar.lasso.cv_selected <- cv.glmnet(x_train_selected, y_train_selected, nfold=5)
plot(potatorar.lasso.cv_selected)
opt_lambda_selected = potatorar.lasso.cv_selected$lambda.min
```

```{r}
potatorar.lasso.cv_selected<- glmnet(x_train_selected, y_train_selected, lambda = opt_lambda_selected)
pred_selected <- predict(potatorar.lasso.cv_selected, s = opt_lambda_selected, newx = x_test_selected)
final_selected <- cbind(y_test_selected, pred_selected)
final_selected
```
```{r}
#MSE
sum_selected = sum((y_test_selected - pred_selected)^2)
#AVG - test mean square error
mse_selected_value <- mean((y_test_selected - pred_selected)^2)
mse_selected_value
```

